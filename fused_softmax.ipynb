{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb89b8ce-3695-4c37-98bb-7674fbf0df22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "DEVICE = torch.device(f'cuda:{torch.cuda.current_device()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc28464-dbe4-497a-b174-18b68a5134a1",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j=1}^n e^{x_j}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1893a94a-0b92-42a7-ae11-e433c377dca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_softmax(x):\n",
    "    # reads MN elements and write M elements\n",
    "    x_max = x.max(dim=1)[0] # shape (M) - grab only the maxes\n",
    "    # read MN + M elements, subtraction is MN flops, write MN elements\n",
    "    z = x - x_max[:, None] # shape (M, N) - shape (M, 1) = shape (M, N)\n",
    "\n",
    "    # reading MN elements and writing MN elements\n",
    "    numerator = torch.exp(z) # shape (M, N)\n",
    "\n",
    "    # read MN elements, then MN flops, write M elements\n",
    "    denominator = numerator.sum(1) # shape (M, N) -> shape (M)\n",
    "\n",
    "    # read MN + M elements, division MN flops, write MN elements\n",
    "    out = numerator / denominator[:, None] # shape (M, N) / shape (M, 1) = shape (M, N)\n",
    "\n",
    "    # in total 8MN + 4M operations\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4969ba50-4863-4c8b-aec9-2379c4095fd2",
   "metadata": {},
   "source": [
    "The goal is instead to read MN only once and write once, meaning we only have MN and the rest are done on DRAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b2fa3a5-7ce8-436d-bcff-efb948302d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_softmax_kernel(size: tuple, atol=1e-3, rtol=1e-2, device=DEVICE):\n",
    "    torch.manual_seed(0)\n",
    "    x = torch.randn(size[0], size[1], device=DEVICE)\n",
    "\n",
    "    z_tri = softmax(x)\n",
    "    z_ref = torch.softmax(x, axis=1)\n",
    "\n",
    "    torch.testing.assert_close(z_tri, z_ref, atol=atol, rtol=rtol)\n",
    "    print(\"PASSED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "013015e0-d475-4d94-9f29-f385f9dd59b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_shared_mem': 101376, 'max_num_regs': 65536, 'multiprocessor_count': 36, 'warpSize': 32, 'sm_clock_rate': 1605000, 'mem_clock_rate': 8001000, 'mem_bus_width': 128}\n"
     ]
    }
   ],
   "source": [
    "properties = triton.runtime.driver.active.utils.get_device_properties(DEVICE.index)\n",
    "NUM_SM = properties[\"multiprocessor_count\"]\n",
    "NUM_REGS = properties[\"max_num_regs\"]\n",
    "TOTAL_SRAM_PER_SM = properties[\"max_shared_mem\"]\n",
    "WARP_SIZE = properties[\"warpSize\"]\n",
    "\n",
    "print(properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5cffda90-7174-48b2-8cad-68203227c948",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    assert x.ndim == 2\n",
    "    assert x.is_contiguous()\n",
    "    \n",
    "    n_rows, n_cols = x.shape\n",
    "    BLOCK_SIZE = triton.next_power_of_2(n_cols)\n",
    "\n",
    "    num_warps = 4\n",
    "    if BLOCK_SIZE >= 2048:\n",
    "        num_warps = 8\n",
    "    if BLOCK_SIZE >= 4096:\n",
    "        num_warps = 16\n",
    "\n",
    "    num_stages = 4 if TOTAL_SRAM_PER_SM > 200_000 else 2\n",
    "\n",
    "    y = torch.empty_like(x)\n",
    "\n",
    "    # Warmup, precompile kernel\n",
    "    kernel = _softmax_kernel.warmup(\n",
    "        x, y,\n",
    "        x.stride(0), y.stride(0),\n",
    "        n_rows, n_cols,\n",
    "        BLOCK_SIZE=BLOCK_SIZE,\n",
    "        num_stages=num_stages,\n",
    "        num_warps=num_warps,\n",
    "        grid=(1,)\n",
    "    )\n",
    "    kernel._init_handles()\n",
    "    n_regs_per_program = kernel.n_regs\n",
    "    sram_needed_per_program = kernel.metadata.shared\n",
    "\n",
    "    reg_occupancy = NUM_REGS // (n_regs_per_program * WARP_SIZE * num_warps)\n",
    "    # NUM_REGS = 65536\n",
    "    # Each program might use\n",
    "        # n_regs_per_program = 32\n",
    "        # WARP_SIZE = 32\n",
    "        # num_warps = 8\n",
    "    # so, each program needs (n_regs_per_program * WARP_SIZE * num_warps) registers toatl\n",
    "    # 65536//(32 * 32 * 8) = 8 program per SM\n",
    "\n",
    "    sram_occupancy = TOTAL_SRAM_PER_SM // sram_needed_per_program\n",
    "    \n",
    "    programs_per_sm = min(reg_occupancy, sram_occupancy) # choose the limiting occupancy\n",
    "\n",
    "    num_programs = min(NUM_SM * programs_per_sm, n_rows) # handle fewer rows than programs needed\n",
    "\n",
    "    grid = (num_programs, 1, 1)\n",
    "\n",
    "    kernel[grid](\n",
    "        x, y,\n",
    "        x.stride(0), y.stride(0),\n",
    "        n_rows, n_cols,\n",
    "        BLOCK_SIZE,\n",
    "        num_stages\n",
    "    )\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87d8b9d4-fc06-4051-b00d-8682a9aa6ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def _softmax_kernel(\n",
    "    input_ptr, output_ptr,\n",
    "    input_row_stride, output_row_stride,\n",
    "    n_rows, n_cols, \n",
    "    BLOCK_SIZE: tl.constexpr,\n",
    "    num_stages: tl.constexpr,\n",
    "):\n",
    "    # shape (M, N)\n",
    "    # BLOCK_SIZE = next power of 2 bigger than N, fitting an entire row into SRAM at a time\n",
    "    # each PID is going to start a different row\n",
    "\n",
    "    PID = tl.program_id(0)\n",
    "    row_step = tl.num_programs(0) # handle more rows than can be run at a time\n",
    "\n",
    "    for row_idx in tl.range(PID, n_rows, row_step, num_stages=num_stages):\n",
    "        row_start_ptr = input_ptr + (row_idx * input_row_stride)\n",
    "        col_offsets = tl.arange(0, BLOCK_SIZE)\n",
    "        input_ptrs = row_start_ptr + col_offsets   # bunch of elements of our input tensor\n",
    "        mask = col_offsets < n_cols                # limit access to overwriting rows\n",
    "        row = tl.load(input_ptrs, mask=mask, other=float('-inf')) # shape (BLOCK_SIZE) ~= shape (n_cols)\n",
    "            # only memory read\n",
    "\n",
    "        row_minus_max = row - tl.max(row, axis=0) # shape (BLOCK_SIZE) - (1) -> (BLOCK_SIZE)\n",
    "        numerator = tl.exp(row_minus_max)         # shape (BLOCK_SIZE)\n",
    "        denominator = tl.sum(numerator, axis=0)   # shape (1)\n",
    "        softmax_output = numerator / denominator  # shape (BLOCK_SIZE) / (1) -> (BLOCK_SIZE)\n",
    "\n",
    "        output_row_start_ptr = output_ptr + row_idx * output_row_stride\n",
    "        tl.store(output_row_start_ptr + col_offsets, softmax_output, mask=mask)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d8ebf79-73ba-473b-ab51-7d065f6c17f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.testing.perf_report(\n",
    "    triton.testing.Benchmark(\n",
    "        x_names=['N'],\n",
    "        x_vals=[128*i for i in range(2, 30)],\n",
    "        line_arg='provider',\n",
    "        line_vals=['triton', 'torch'],\n",
    "        line_names=['Triton', 'Torch'],\n",
    "        styles=[('blue', '-'), ('green', '-')],\n",
    "        ylabel=\"GB/s\",\n",
    "        plot_name=\"softmax-performance\",\n",
    "        args={'M': 4096}\n",
    "    )\n",
    ")\n",
    "def benchmark(M, N, provider):\n",
    "    x = torch.randn(M, N, device=DEVICE, dtype=torch.float32)\n",
    "    \n",
    "    stream = getattr(torch, DEVICE.type).Stream()\n",
    "    getattr(torch, DEVICE.type).set_stream(stream)\n",
    "\n",
    "    if provider == 'torch':\n",
    "        ms = triton.testing.do_bench(lambda: torch.softmax(x, axis=-1))\n",
    "    if provider == 'triton':\n",
    "        ms = triton.testing.do_bench(lambda: softmax(x)) \n",
    "\n",
    "    gbps = lambda ms: 2 * x.numel() * x.element_size() * 1e-9 / (ms * 1e-3)\n",
    "    return gbps(ms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25dce86e-e701-4ccc-b708-74d4408ffb40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PASSED\n"
     ]
    }
   ],
   "source": [
    "test_softmax_kernel(size=(1823, 781))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038ca96b-eabe-4eab-a224-9b0115ea4efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark.run(save_path='.', print_data=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
